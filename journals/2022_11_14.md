- [Why does calloc exist?](https://vorpus.org/blog/why-does-calloc-exist/)
	- type:: read
	- calloc(count, size) $\neq$ malloc + memset(0)
	- calloc:
		- Check if count $\times$ size is overflow or not.
		- May not memset(0) if all memory is already "zeroed".
- [[GRPC]]
	- type:: survey
	- Thread Model (Thread Connection Stream Read Write)
		- [grpc::load_reporter::LoadReporterAsyncServiceImpl](https://github.com/grpc/grpc/blob/master/src/cpp/server/load_reporter/load_reporter_async_service_impl.h)
		  collapsed:: true
			- **LoadReporterAsyncServiceImpl**
				- only one instance
				- inherit from a proto gen struct
				- static method: `Work`
					- Start in a thread
					- Main loop: Fetch item from `ServerCompletionQueue`
				- When will this construct?
					- ```cpp
					  grpc::ServerBuilder builder;
					  // ...
					  server_ = builder.BuildAndStart();
					  ```
					- ```cpp
					  // ServerBuilder::BuildAndStart()
					  ChannelArguments args = BuildChannelArgs();
					  ```
					- ```cpp
					  // ServerBuilder::BuildChannelArgs()
					  plugin->UpdateServerBuilder(this);
					  ```
					- ```cpp
					  // LoadReportingServiceServerBuilderPlugin::UpdateServerBuilder
					  auto cq = builder->AddCompletionQueue();
					  service_ = std::make_shared<LoadReporterAsyncServiceImpl>(std::move(cq));
					  ```
			- **CallableTag**
				- run-once-functor
				- store a shared pointer of a function and call it with **ReportLoadHandler**
			- **ReportLoadHandler**
				- take care of one loading stream
				- access parent class **LoadReporterAsyncServiceImpl**
				-
		- grpc::ServerBuilder::BuildAndStart
		  collapsed:: true
			- ```cpp
			  // If this is a Sync server, i.e a server expositing sync API, then the server
			  // needs to create some completion queues to listen for incoming requests.
			  // 'sync_server_cqs' are those internal completion queues.
			  ```
			- ```cpp
			  // cqs_ contains the completion queue added by calling the ServerBuilder's
			  // AddCompletionQueue() API. Some of them may not be frequently polled (i.e by
			  // calling Next() or AsyncNext()) and hence are not safe to be used for
			  // listening to incoming channels. Such completion queues must be registered
			  // as non-listening queues.
			  ```
			- ```cpp
			  server->Start(cqs_data, cqs_.size());
			  ```
		- grpc::Server
		  collapsed:: true
			- ```cpp
			  /// List of \a ThreadManager instances (one for each cq in
			  /// the \a sync_server_cqs)
			  std::vector<std::unique_ptr<SyncRequestThreadManager>> sync_req_mgrs_;
			  ```
			- ```cpp
			  // ctor
			  for (const auto& it : *sync_server_cqs_) {
			    sync_req_mgrs_.emplace_back(new SyncRequestThreadManager(
			      this, it.get(), ...));
			  }
			  ```
		- grpc::Server::Start()
		  collapsed:: true
			- ```cpp
			  grpc_server_start(server_);
			  ```
			- ```cpp
			  // If this server has any support for synchronous methods (has any sync
			  // server CQs), make sure that we have a ResourceExhausted handler
			  // to deal with the case of thread exhaustion
			  ```
		- ## Sync
			- grpc::Server::SyncRequestThreadManager
			  collapsed:: true
				- ```cpp
				  // Implementation of ThreadManager. Each instance of SyncRequestThreadManager
				  // manages a pool of threads that poll for incoming Sync RPCs and call the
				  // appropriate RPC handlers
				  class Server::SyncRequestThreadManager : public grpc::ThreadManager
				  ```
			- grpc::ThreadManager
				- grpc::ThreadManager::WorkerThread
					- ```cpp
					  // ctor
					  // well... just simpify
					  thd_ = grpc_core::Thread("grpcpp_sync_server", this->Run);
					  ```
					- ```cpp
					  // Run()
					  thd_mgr_->MainWorkLoop();
					  ```
				- MainWorkLoop
					- ```cpp
					  // If we got work and there are now insufficient pollers and there is
					  // quota available to create a new thread, start a new poller thread
					  ```
					- Use grpc::ThreadQuota to limit max thread
			- Sync Mode of GRPC:
				- Hold a pool of thread.
				- Use a thread for a stream.
				- $\max|Thread|$ can be controlled by ResourceQuota
			- ![grpc-sync-thread-model.png](../assets/grpc-sync-thread-model_1668420571995_0.png)
		- grpc_server_start
		  id:: 6371efbe-b5ee-41b2-b430-00b7aaa70e6a
			- ```cpp
			  grpc_core::Server::FromC(server)->Start();
			  ```
		- ## TCP
			- ```cpp
			  static void tcp_server_start(grpc_tcp_server* s,
			                               const std::vector<grpc_pollset*>* pollsets,
			                               grpc_tcp_server_cb on_accept_cb,
			                               void* on_accept_cb_arg) {
			    // ...
			    for (i = 0; i < pollsets->size(); i++) {
			      grpc_pollset_add_fd((*pollsets)[i], sp->emfd);
			      GRPC_CLOSURE_INIT(&sp->read_closure, on_read, sp,
			                        grpc_schedule_on_exec_ctx);
			      grpc_fd_notify_on_read(sp->emfd, &sp->read_closure);
			      s->active_ports++;
			      sp = sp->next;
			    }
			    // ...
			  }
			  ```
			- ```cpp
			  // static void on_read(void* arg, grpc_error_handle err) {
			  /* loop until accept4 returns EAGAIN, and then re-arm notification */
			  sp->server->on_accept_cb(
			    sp->server->on_accept_cb_arg,
			    grpc_tcp_create(fdobj, sp->server->options, addr_uri.value()),
			    read_notifier_pollset, acceptor);
			  ```
-